{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a notebook aiming to explore KNN graphs of image embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are familiar with jupyter notebooks please start running the notebook from [section 0](#0-import-dependencies), otherwise the following aims to give you a quickstart guide.\n",
    "\n",
    "1. Jupter notebooks are made up of 'cells,' these are either 'code' (python in this case ), or 'text' (markdown) cells.\n",
    "2. To run a cell you can either click on the run symbol in a cell, or use `shift`+`enter`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import Dependencies\n",
    "This notebook includes many functions in different files, feel free to explore them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os \n",
    "\n",
    "import numpy as np\n",
    "import zipfile\n",
    "\n",
    "### Embedding options ###\n",
    "## VGG16 ##\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import ImageDataset\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from torchvision import models\n",
    "\n",
    "## Plotting ##\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(42) \n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "from get_embeddings import get_clip_embedding, get_normalised_histogram, get_embedding, get_embeddings_pytorch\n",
    "\n",
    "from embeddings_analysis import get_knn, get_count_neighbour_occurances, rank_neighbour_occurances, get_mean_distance, get_cluster_labels\n",
    "\n",
    "from force_directed_diagram import get_diagram_data #, get_position_df\n",
    "\n",
    "from get_html import get_html_rendering_details, generate_html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load images "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please upload a single `.zip` file into the `input` folder [here](./input), and then run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully found file 'pokemon_images.zip'.\n"
     ]
    }
   ],
   "source": [
    "input_files = os.listdir('./input')\n",
    "if len(input_files) == 2:\n",
    "    if not (input_files[0].endswith('.zip') or input_files[1].endswith('.zip')):\n",
    "        print(\"Single file in input folder (excluding .gitkeep) is not a .zip file\")\n",
    "\n",
    "    else:\n",
    "        for file in input_files:\n",
    "            if file.endswith('.zip'):\n",
    "                # therefore there is one zip file \n",
    "                IMAGE_ZIP = \"./input/\" + file\n",
    "                print(f\"Successfully found file '{file}'.\")\n",
    "else:\n",
    "    print(\"One file not found in the input folder (excluding .gitkeep)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may edit `IMAGE_EXTENSIONS` if you wish to exclude/include particular filename suffixes. You may also edit `IMAGE_IGNORE` to exclude certain filename prefixes (e.g. `._` is a metadata filename prefix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "809 images detected.\n"
     ]
    }
   ],
   "source": [
    "# adjust these to desired values (Zip file images will be read from)\n",
    "IMAGE_EXTENSIONS = (\".jpg\", \".JPG\", \".png\",\".PNG\", \".JPEG\", \".jpeg\") # The filename suffixes to be included\n",
    "IMAGE_IGNORE = (\"._\", ) # filename prefixes to be excluded \"._\" is for metadata\n",
    "\n",
    "# Display how many images are detected in the zip file.\n",
    "imgzip = zipfile.ZipFile(IMAGE_ZIP)\n",
    "print(f\"{len([name for name in imgzip.namelist() if name.endswith(IMAGE_EXTENSIONS) and not name.split('/')[-1].startswith(IMAGE_IGNORE)])} images detected.\")\n",
    "del imgzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example code for displaying an image from the zip file. The following code displays the first image in the zip file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pokemon_images/parasect.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGBA size=120x120>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imgzip = zipfile.ZipFile(IMAGE_ZIP)\n",
    "for name in imgzip.namelist():\n",
    "    if name.endswith(IMAGE_EXTENSIONS) and not name.split(\"/\")[-1].startswith(IMAGE_IGNORE):\n",
    "        with Image.open(imgzip.open(name)) as img:\n",
    "            print(name)\n",
    "            display(img)\n",
    "            break\n",
    "del imgzip, name, img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Embed images\n",
    "In this section we go over methods to convert our images into a 1D array of numbers (or an N-dimensional vector).\n",
    "\n",
    "The following are simply examples, and you are more than welcome to adjust the existing code to fit your needs, or to import your own embeddings. The format is a 2D array, where each row is a different image (with the ith row in `embedding` corresponding to the ith filename in `image_filenames`).\n",
    "\n",
    "The custom functions described in this section can be found in [get_embeddings.py](./get_embeddings.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Basic no-batch embedding\n",
    "The following provides code examples for how to do simple no-batch embedding. This means each image is iterated over individually. This will work in almost all cases, however may not be the most efficient. The `get_embedding` function does utilise multithreading.\n",
    "\n",
    "The following is an example of how to use the `get_embedding` function. It takes 3 parameters, the two constants defined above, and an `embedding_function`. This function must take in a `PIL.Image` object and return a 1D array of floats (it is crucial that this array is the same length for every image).\n",
    "\n",
    "The example `embedding_function` provided is `get_normalised_histogram`, but this can be swapped out to whatever you wish as long as it fulfills the above criteria.\n",
    "\n",
    "(This code is not crucial to the notebook, so you are welcome to set `skip` to `True` if you wish.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 809/809 [00:00<00:00, 4790.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.39791667e-01 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 1.02083333e-02]\n",
      " [8.62916667e-01 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  1.45833333e-03 1.06250000e-02]\n",
      " [7.97013889e-01 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 2.85416667e-02]\n",
      " [9.01736111e-01 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 1.11111111e-03]\n",
      " [7.38402778e-01 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 8.33333333e-04]] ['pokemon_images/parasect.png' 'pokemon_images/lumineon.png'\n",
      " 'pokemon_images/raikou.png' 'pokemon_images/dedenne.png'\n",
      " 'pokemon_images/pyroar.png']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# This should take about 15 seconds to run for 1000 images\n",
    "skip = False\n",
    "if not skip:\n",
    "    embedding_function = get_normalised_histogram\n",
    "    embedding, image_filenames = get_embedding(image_zip=IMAGE_ZIP, \n",
    "                                            image_extensions=IMAGE_EXTENSIONS, \n",
    "                                            image_ignore=IMAGE_IGNORE,\n",
    "                                            embedding_function=embedding_function) \n",
    "    print(embedding[0:5], image_filenames[0:5])\n",
    "    del embedding, image_filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Batch embedding (pytorch)\n",
    "The following will describe how to utilise the existing code for pytorch batch embedding. \n",
    "\n",
    "This requires a custom dataset class and dataloader (defined below). It is recommended to play with the `DataLoader` parameters to see what runs best. If you use a pytorch embedder other than one of the VGG's (e.g. VGG16/VGG19) you may have to adjust the `ImageDataset` `transform` attribute to resize to an appropriate size with an appropriate mean and std of that embedder.\n",
    "\n",
    "The cell defining dataset and dataloader must be run, however the one following it is an example implementation of the `get_embeddings_pytorch` function and may be skipped by setting `skip` to `True` (This is as this may take a while to run and it will be run later anyway). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import custom tensor dataset from other file\n",
    "dataset = ImageDataset(image_zip=IMAGE_ZIP, image_extensions=IMAGE_EXTENSIONS, image_ignore=IMAGE_IGNORE,\n",
    "                       transform= transforms.Compose([\n",
    "                           transforms.Resize((224, 224)),  # Resize image to 224x224 pixels\n",
    "                           transforms.ToTensor(),         # Convert image to tensor\n",
    "                           transforms.Normalize(          # Normalize using ImageNet statistics\n",
    "                            mean=[0.485, 0.456, 0.406],  # Mean for RGB channels\n",
    "                            std=[0.229, 0.224, 0.225]    # Standard deviation for RGB channels\n",
    "                                )\n",
    "    ]))\n",
    "\n",
    "# can look into optimising these values\n",
    "dataloader = DataLoader(dataset, batch_size=64,\n",
    "                        shuffle=False, num_workers=0) #,prefetch_factor=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is skipped by default as 1. It takes a while to run and 2. We will be running it later to produce some of the embeddings.\n",
    "\n",
    "Note: to adjust the model you may simply swap vgg16 to another pytorch model, listed here: https://pytorch.org/vision/main/models.html\n",
    "(Please keep in mind the `ImageDataset` `transform`s may need to be changed if you use a non VGG model, so please refer to the documentation for that model.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should take about 13 minutes to run for 1000 images\n",
    "skip = True\n",
    "if not skip:\n",
    "    pytorch_model = models.vgg16(weights=models.VGG16_Weights.DEFAULT).to(\"cpu\")\n",
    "    batch_embeddings, image_filenames = get_embeddings_pytorch(dataloader, pytorch_model=pytorch_model)\n",
    "\n",
    "    print(embedding[0:5], image_filenames[0:5])\n",
    "    del embedding, image_filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Run all embeddings\n",
    "This cell runs the 4 default embeddings setup for this notebook. You are more than welcome to adjust this. The CLIP embedding is utilising the `imgbeddings` library: https://github.com/minimaxir/imgbeddings\n",
    "\n",
    "\n",
    "The following cell may be commented out after running once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 809/809 [00:00<00:00, 3794.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 356 ms, sys: 122 ms, total: 478 ms\n",
      "Wall time: 268 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# This should take about 15 seconds to run for 1000 images\n",
    "#### Get embedding without batching ####\n",
    "embedding_function = get_normalised_histogram\n",
    "embedding, image_filenames = get_embedding(image_zip=IMAGE_ZIP, image_extensions=IMAGE_EXTENSIONS, image_ignore=IMAGE_IGNORE, embedding_function=embedding_function) \n",
    "np.save(\"./output/image_embedding_hist.npy\", embedding)\n",
    "np.save(\"./output/image_filenames_hist.npy\", image_filenames) # this is to keep a consistent log of the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total batches to process: 13\n",
      "Processing batch 1/13...\n",
      "Processing batch 2/13...\n",
      "Processing batch 3/13...\n",
      "Processing batch 4/13...\n",
      "Processing batch 5/13...\n",
      "Processing batch 6/13...\n",
      "Processing batch 7/13...\n",
      "Processing batch 8/13...\n",
      "Processing batch 9/13...\n",
      "Processing batch 10/13...\n",
      "Processing batch 11/13...\n",
      "Processing batch 12/13...\n",
      "Processing batch 13/13...\n",
      "\n",
      "Batch processing complete.\n",
      "CPU times: user 5min 27s, sys: 33.4 s, total: 6min 1s\n",
      "Wall time: 1min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# This should take about 13 minutes to run for 1000 images\n",
    "#### Get embedding with batching (for pytorch) #####\n",
    "# https://pytorch.org/vision/main/models/vgg.html\n",
    "# https://pytorch.org/vision/main/models/generated/torchvision.models.vgg16.html#torchvision.models.VGG16_Weights \n",
    "\n",
    "# VGG19\n",
    "pytorch_model = models.vgg19(weights=models.VGG19_Weights.DEFAULT).to(\"cpu\")\n",
    "batch_embeddings, image_filenames = get_embeddings_pytorch(dataloader, pytorch_model=pytorch_model)\n",
    "np.save(\"./output/image_embedding_vgg19.npy\", batch_embeddings)\n",
    "np.save(\"./output/image_filenames_vgg19.npy\", image_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total batches to process: 13\n",
      "Processing batch 1/13...\n",
      "Processing batch 2/13...\n",
      "Processing batch 3/13...\n",
      "Processing batch 4/13...\n",
      "Processing batch 5/13...\n",
      "Processing batch 6/13...\n",
      "Processing batch 7/13...\n",
      "Processing batch 8/13...\n",
      "Processing batch 9/13...\n",
      "Processing batch 10/13...\n",
      "Processing batch 11/13...\n",
      "Processing batch 12/13...\n",
      "Processing batch 13/13...\n",
      "\n",
      "Batch processing complete.\n",
      "CPU times: user 4min 32s, sys: 29.5 s, total: 5min 1s\n",
      "Wall time: 1min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# This should take about 12 minutes to run for 1000 images\n",
    "#### Get embedding with batching (for pytorch) #####\n",
    "# https://pytorch.org/vision/main/models/vgg.html\n",
    "# https://pytorch.org/vision/main/models/generated/torchvision.models.vgg16.html#torchvision.models.VGG16_Weights \n",
    "\n",
    "# VGG16\n",
    "pytorch_model = models.vgg16(weights=models.VGG16_Weights.DEFAULT).to(\"cpu\")\n",
    "batch_embeddings, image_filenames = get_embeddings_pytorch(dataloader, pytorch_model=pytorch_model)\n",
    "np.save(\"./output/image_embedding_vgg16.npy\", batch_embeddings)\n",
    "np.save(\"./output/image_filenames_vgg16.npy\", image_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasperchong/opt/miniconda3/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'cached_download' (from 'huggingface_hub.file_download') is deprecated and will be removed from version '0.26'. Use `hf_hub_download` instead.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "/Users/jasperchong/opt/miniconda3/lib/python3.9/site-packages/huggingface_hub/file_download.py:672: FutureWarning: 'cached_download' is the legacy way to download files from the HF hub, please consider upgrading to 'hf_hub_download'\n",
      "  warnings.warn(\n",
      "/Users/jasperchong/opt/miniconda3/lib/python3.9/site-packages/transformers/models/clip/processing_clip.py:149: FutureWarning: `feature_extractor` is deprecated and will be removed in v5. Use `image_processor` instead.\n",
      "  warnings.warn(\n",
      "100%|██████████| 25/25 [00:35<00:00,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 15s, sys: 1.85 s, total: 2min 17s\n",
      "Wall time: 39.8 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# This should take about 6 minutes to run for 1000 images\n",
    "### Get embedding CLIP (inbuilt batching) #### \n",
    "clip_embeddings, image_filenames = get_clip_embedding(IMAGE_ZIP, IMAGE_EXTENSIONS, image_ignore=IMAGE_IGNORE, batch_size=32)\n",
    "np.save(\"./output/image_embedding_clip.npy\", clip_embeddings)\n",
    "np.save(\"./output/image_filenames_clip.npy\", image_filenames) # this is to keep a consistent log of the index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Load selected embeddings\n",
    "You may change the value of `embedding_to_load` to load a specific embedding of your choice. This choice will be used when walking through helper functions and visualisations in this notebook.\n",
    "\n",
    "Please note in section 4 of this notebook there is an option to either generate an output HTML file for just the embedding selected here, or for any number of them at once. If you opt for the second option and don't wish to see examples, you may [skip to generating html](#4-html-export) from here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 988 µs, sys: 3.19 ms, total: 4.18 ms\n",
      "Wall time: 7.79 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# this may be either \"hist\", \"vgg16\", \"vgg19\", or \"clip\"\n",
    "embedding_to_load = \"hist\" \n",
    "#### Load Embeddings ####\n",
    "## Colour Histogram ##\n",
    "if embedding_to_load == \"hist\":\n",
    "    embedding = np.load(\"./output/image_embedding_hist.npy\")\n",
    "    image_filenames = np.load(\"./output/image_filenames_hist.npy\")\n",
    "## VGG16 ##\n",
    "elif embedding_to_load == \"vgg16\":\n",
    "    embedding = np.load(\"./output/image_embedding_vgg16.npy\")\n",
    "    image_filenames = np.load(\"./output/image_filenames_vgg16.npy\")\n",
    "## VGG19 ##\n",
    "elif embedding_to_load == \"vgg19\":\n",
    "    embedding = np.load(\"./output/image_embedding_vgg19.npy\")\n",
    "    image_filenames = np.load(\"./output/image_filenames_vgg19.npy\")\n",
    "## CLIP ##\n",
    "elif embedding_to_load == \"clip\":\n",
    "    embedding = np.load(\"./output/image_embedding_clip.npy\")\n",
    "    image_filenames = np.load(\"./output/image_filenames_clip.npy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Embedding Analysis\n",
    "In this section we will define some functions to help us analyse the image embeddings. The custom functions described in this section can be found in [embeddings_analysis.py](./embeddings_analysis.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Get KNN\n",
    "In this section we generate the `nearest_neighbours_array` and `neighbours_distance_array` for the desired `number_of_neighbours`. \n",
    "The `nearest_neighbours_array` is a 2D array where the ith row represents the ith filename in `image_filenames`, and each row lists the desired N nearest neighbours for that specific image.\n",
    "The `neighbours_distance_array` is similar to the `nearest_neighbours_array`, however lists the neigbour distance.\n",
    "\n",
    "Below is an example implementation, it shouldn't take long to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.96 s, sys: 405 ms, total: 7.36 s\n",
      "Wall time: 8.18 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[  0, 561,  27, 796, 534, 170, 711, 758, 139, 730],\n",
       "        [  1, 669, 434, 658, 577, 247, 286, 489, 381, 640],\n",
       "        [  2,  17, 551,  86, 530, 557, 804, 656, 598,  93]], dtype=int32),\n",
       " array([[0.        , 0.06797501, 0.06986193, 0.06989838, 0.07062121,\n",
       "         0.07697889, 0.07788243, 0.07809988, 0.07863939, 0.07905176],\n",
       "        [0.        , 0.06143577, 0.0615523 , 0.06310952, 0.06361414,\n",
       "         0.06384276, 0.0638935 , 0.06402875, 0.06431576, 0.06553937],\n",
       "        [0.        , 0.04600714, 0.05792491, 0.06194437, 0.06298876,\n",
       "         0.0649245 , 0.0650405 , 0.06605449, 0.06925914, 0.07098201]],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# This should take about 30 seconds to run for 1000 images\n",
    "number_of_neighbours = 10\n",
    "nearest_neighbours_array, neighbours_distance_array = get_knn(embedding, number_of_neighbours)\n",
    "\n",
    "nearest_neighbours_array[0:3], neighbours_distance_array[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Make some analysis helper functions\n",
    "The first helper function is `get_count_neighbour_occurances`. This function takes the `nearest_neighbour_arary` and returns an array where the ith index is how many times that index appeared as a neighbour (including as a neighbour of itself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5,  1, 23, 18,  9,  3,  4,  6,  1,  9])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_neighbour_occurance_array = get_count_neighbour_occurances(nearest_neighbours_array=nearest_neighbours_array)\n",
    "count_neighbour_occurance_array[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up we have `rank_neighbour_occurances`, which takes the previously defined `count_neighbour_occurance_array` as an input, and returns an array where the ith value in the array indicates that the index stored there is the ith most common or ith least common neighbour, depending on if the `ascending` parameter is set to `True` or `False` (`True` by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common neighbour is at image_filenames index 736\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([188, 269, 309, 110, 345, 344, 712, 341, 275, 484])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_neighbour_occurance_array = rank_neighbour_occurances(count_neighbour_occurance_array, ascending=True)\n",
    "print(f\"The most common neighbour is at image_filenames index {ranked_neighbour_occurance_array[-1]}\") # The last value as ascending\n",
    "ranked_neighbour_occurance_array[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_mean_distance` takes in the previously described `neighbours_distance_array` and returns a 1D array where the ith index indicates the ith images mean distance from all of its N nearest neighburs. You may choose whether this calculation should include itself as a neighbour (as each image is always its own closest neighbour) by setting the boolean value of `exclude_self` (`True` by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_mean_distance(neighbours_distance_array, exclude_self=True)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Getting K-means clusters\n",
    "These will be used as an additional visualisation option for the images.\n",
    "\n",
    "The `get_cluster_labels` function takes in `embedding` (the image embedding array) and `n_clusters` (desired number of clusters, by default 10). If you wish you may also change the `random_state`. This function returns a 1D array, where the ith index indicates what cluster the ith image lies in. (e.g. 0th cluster, 1st cluster...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cluster_labels = get_cluster_labels(embedding, n_clusters=20)\n",
    "cluster_labels[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML GENERATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. HTML Export \n",
    "Create a \"summary\" HTML document. What this document contains is better described within the file. If running in binderhub, please right click + `Open in New Browser Tab` the html file to ensure the javascript runs as expected. \n",
    "\n",
    "The main options are the following: `colour_map` (the variable that describes how different clusters are coloured) is set to `\"tab20\"` by default but it may be set to any of the options avaliable when running `from matplotlib import colormaps` and `list(colormaps)` (for over 20 clusters you would need to use a continous map like `\"viridis\"`).\n",
    "\n",
    "You may also select if you would like your images to be grouped by cluster when displayed. The `group_cluster` parameter has the following options:\n",
    "- `False`: Sort by rank normally\n",
    "- `True`: Order by Cluster label order\n",
    "- `asc`: Sort by ascending cluster size order\n",
    "- `desc`: Sort by descending cluster size order\n",
    "- `rank`: Sort by the highest ranked image in each cluster\n",
    "\n",
    "Please note the HTML is fully self contained, meaning after it is created it has the images and required packages encoded within it. This is so the HTML can be easily shared, and will still be runnable without internet access.\n",
    "\n",
    "The custom functions described in this section can be found in [get_html.py](./get_html.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell you can change the boolean value of `generate_all` to select one of the following:\n",
    "- `True`: generate a html file for each embedding in `embedding_name_list`.Note that the name in `embedding_name_list` refers to the name used when saving the embedding + image_filenames. This can be run immediately after saving the embeddings, and does not require any of the cells inbetween. \n",
    "\n",
    "- `False`: generate a single html file using the values from sections 2 and 3. This means it will use the embedding type you chose to load, the number of clusters or nearest neighbours you selected etc. This option requires running the cells up to this point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnidentifiedImageError",
     "evalue": "cannot identify image file <zipfile.ZipExtFile name='__MACOSX/pokemon_images/._parasect.png' mode='r' compress_type=deflate>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:17\u001b[0m\n",
      "File \u001b[0;32m~/PycharmProjects/Legacy coding/exploring-knn/get_html.py:152\u001b[0m, in \u001b[0;36mgenerate_html\u001b[0;34m(nearest_neighbours_array, neighbours_distance_array, image_zip, image_extensions, diagram_data, cluster_labels, group_clusters, colour_map)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m im_b64\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    151\u001b[0m diagram_data_dict \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(diagram_data)\n\u001b[0;32m--> 152\u001b[0m diagram_data_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_filenames\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [get_resized_b64(\n\u001b[1;32m    153\u001b[0m     Image\u001b[38;5;241m.\u001b[39mopen(imgzip\u001b[38;5;241m.\u001b[39mopen(filename))\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m image_filenames]\n\u001b[1;32m    154\u001b[0m diagram_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps(diagram_data_dict)\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m diagram_data_dict\n",
      "File \u001b[0;32m~/PycharmProjects/Legacy coding/exploring-knn/get_html.py:153\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m im_b64\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    151\u001b[0m diagram_data_dict \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(diagram_data)\n\u001b[1;32m    152\u001b[0m diagram_data_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_filenames\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [get_resized_b64(\n\u001b[0;32m--> 153\u001b[0m     \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgzip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m image_filenames]\n\u001b[1;32m    154\u001b[0m diagram_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps(diagram_data_dict)\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m diagram_data_dict\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/PIL/Image.py:3536\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3534\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message)\n\u001b[1;32m   3535\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot identify image file \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (filename \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;28;01melse\u001b[39;00m fp)\n\u001b[0;32m-> 3536\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m UnidentifiedImageError(msg)\n",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m: cannot identify image file <zipfile.ZipExtFile name='__MACOSX/pokemon_images/._parasect.png' mode='r' compress_type=deflate>"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "generate_all = True\n",
    "# generate_all = True: Should take about 4 minutes to run for 1000 images and 4 embeddings.\n",
    "# generate_all = False: Should take about 1 minute to run for 1000 images and 1 embedding.\n",
    "## Settings for both options ##\n",
    "group_clusters=True\n",
    "colour_map=\"tab20\"\n",
    "\n",
    "## generate_all=True specific options ## \n",
    "# Please ensure that the embeddings have been created for all embeddings in the list\n",
    "embedding_name_list = ['hist', 'vgg19','vgg16', 'clip']\n",
    "number_of_neighbours=10\n",
    "n_clusters=20\n",
    "\n",
    "\n",
    "if generate_all:\n",
    "    for embedding_name in embedding_name_list:\n",
    "        html_file = generate_html(*get_html_rendering_details(embedding_name, IMAGE_ZIP, IMAGE_EXTENSIONS, IMAGE_IGNORE, number_of_neighbours=number_of_neighbours, n_clusters=n_clusters), \n",
    "                group_clusters=group_clusters, \n",
    "                colour_map=colour_map)\n",
    "\n",
    "        with open(f\"./output/output_{embedding_name}.html\",'w') as file:\n",
    "            file.write(html_file)\n",
    "            del html_file, file\n",
    "\n",
    "        print(f\"Completed export for {embedding_name}\")\n",
    "\n",
    "\n",
    "elif not generate_all:\n",
    "    diagram_data = get_diagram_data(nearest_neighbours_array, neighbours_distance_array, cluster_labels, image_filenames, return_json=True)\n",
    "\n",
    "    html_file = generate_html(nearest_neighbours_array, neighbours_distance_array, IMAGE_ZIP, IMAGE_EXTENSIONS, IMAGE_IGNORE, diagram_data, cluster_labels, group_clusters=group_clusters, colour_map=colour_map)\n",
    "\n",
    "    with open(\"./output/output.html\",'w') as file:\n",
    "        file.write(html_file)\n",
    "\n",
    "    del html_file, diagram_data, file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have generated the html output files, they can be found in [./output](./output). If running in binderhub, please right click + `Open in New Browser Tab` the html file to ensure the javascript runs as expected. Or you may download them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Data exploration\n",
    "This following section aims to serve as an example of some visualisation options. Feel free to add your own here. \n",
    "Please note this section depends on the running of sections 2 and 3.\n",
    "\n",
    "First, lets graph the most common neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Most common neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_neighbour_occurances_array = get_count_neighbour_occurances(nearest_neighbours_array)\n",
    "ranked_neighbour_occurances = rank_neighbour_occurances(count_neighbour_occurances_array, ascending=False)\n",
    "\n",
    "n_most_common = 10\n",
    "x=[image_filenames[idx] for idx in ranked_neighbour_occurances[0:n_most_common]]\n",
    "y=[count_neighbour_occurances_array[idx] -1 for idx in ranked_neighbour_occurances[0:n_most_common]] # the -1 is to exclude counting being its own neighbour\n",
    "del count_neighbour_occurances_array, ranked_neighbour_occurances\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "sns.barplot(x=x, y=y)\n",
    "\n",
    "plt.xticks(rotation=270)\n",
    "plt.xlabel(\"Image names\")\n",
    "plt.ylabel(\"Neighbour Counts (excluding being its own neighbour)\")\n",
    "plt.title(f\"{n_most_common} most common image neighbours (descending) for 10 nearest neighbours (9 excluding self)\")\n",
    "\n",
    "plt.show()\n",
    "print(x[0]) # the filename for the most common image "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the most common image neighbour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgzip = zipfile.ZipFile(IMAGE_ZIP)\n",
    "with Image.open(imgzip.open(x[0])) as img:\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Closest image to its neighbours (average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours_mean_distance_array = get_mean_distance(neighbours_distance_array)\n",
    "ranked_neighbour_occurances = rank_neighbour_occurances(neighbours_mean_distance_array, ascending=True)\n",
    "\n",
    "n_most_common = 10\n",
    "x=[image_filenames[idx] for idx in ranked_neighbour_occurances[0:n_most_common]]\n",
    "y=[neighbours_mean_distance_array[idx] for idx in ranked_neighbour_occurances[0:n_most_common]] # the -1 is to exclude counting being its own neighbour\n",
    "del neighbours_mean_distance_array, ranked_neighbour_occurances\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "sns.barplot(x=x, y=y)\n",
    "\n",
    "plt.xticks(rotation=270)\n",
    "plt.xlabel(\"Image names\")\n",
    "plt.ylabel(\"Mean distance from neighbours (excluding itself)\")\n",
    "plt.title(f\"{n_most_common} images with the smallest mean distance from 10 nearest neighbours (excluding self)\")\n",
    "\n",
    "plt.show()\n",
    "print(x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the closest image neighbour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgzip = zipfile.ZipFile(IMAGE_ZIP)\n",
    "with Image.open(imgzip.open(x[0])) as img:\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Histogram for times an image appears as a nearest neighbour\n",
    "This graph is worth noting in particular for different embeddings, some have much more even distributions than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_count_neighbour_occurances_array = np.sort(get_count_neighbour_occurances(nearest_neighbours_array))-1 # -1 to exclude self\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "sns.histplot(x=sorted_count_neighbour_occurances_array, kde=True, bins=30)\n",
    "plt.xticks(rotation=270)\n",
    "plt.xlabel(f\"Number of nearest neighbour occurances excluding self ({number_of_neighbours} nearest neighbours)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(f\"Nearest neighbour count histogram for {number_of_neighbours} nearest neighbours\")\n",
    "\n",
    "plt.show()\n",
    "del sorted_count_neighbour_occurances_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Histogram for an images mean distance across its neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = False\n",
    "\n",
    "if log:\n",
    "    sorted_mean_distance_array = np.log(np.sort(get_mean_distance(neighbours_distance_array)))\n",
    "    title = \"Log mean\"\n",
    "else:\n",
    "    sorted_mean_distance_array = np.sort(get_mean_distance(neighbours_distance_array))\n",
    "    title = \"Mean\"\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "sns.histplot(x=sorted_mean_distance_array, kde=True, bins=30)\n",
    "\n",
    "plt.xticks(rotation=270)\n",
    "plt.xlabel(f\"{title} distance from nearest neighbours excluding self ({number_of_neighbours} nearest neighbours)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(f\"{title} neighbour distance histogram for {number_of_neighbours} nearest neighbours\")\n",
    "\n",
    "plt.show()\n",
    "del sorted_mean_distance_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Visualise the embeddings for the most and least common nearest neighbours\n",
    "\n",
    "A bar graph to physically view the differences between the most and least common image neighbours. Please note these graphs don't always work due to handling of extreme values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_neighbour_occurances_array = get_count_neighbour_occurances(nearest_neighbours_array)\n",
    "max_index = np.argmax(count_neighbour_occurances_array)\n",
    "\n",
    "plt.bar(np.arange(len(embedding[max_index])), embedding[max_index])\n",
    "plt.title(\"Embedding for highest neighbour image\")\n",
    "plt.show()\n",
    "print(image_filenames[max_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_index = np.argmin(count_neighbour_occurances_array)\n",
    "\n",
    "plt.bar(np.arange(len(embedding[min_index])), embedding[min_index])\n",
    "plt.title(\"Embedding for lowest neighbour image\")\n",
    "plt.show()\n",
    "del count_neighbour_occurances_array\n",
    "print(image_filenames[min_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Force directed diagram code\n",
    "This section was originally to create an interactive \"network plot\" with a heatmap for aid in visualisation, using different colours for each points cluster. \n",
    "\n",
    "The code for viewing these plots can be seen commented out in [force_directed_diagram.py](./force_directed_diagram.py), however the same visualisation are avaliable in the output html documents [created in section 4](#4-html-export) (**this is the recommended way to view these graphs**).\n",
    "\n",
    "The example code shown in this section is use to create the diagrams for the output html file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_diagram_data` function returns all the data needed to construct the force directed diagram, taking in a variety of previously defined data. Setting `return_json` to `True` will return a string (this is used for generating the HTML)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagram_data = get_diagram_data(nearest_neighbours_array, neighbours_distance_array, cluster_labels, image_filenames, return_json=False)\n",
    "print(diagram_data.keys()) # showing keys as the actual dictionary is very large\n",
    "del diagram_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7 Pie chart for images in each cluster\n",
    "This chart was created to help visualise the differences in cluster sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, counts = np.unique(cluster_labels, return_counts=True) \n",
    "\n",
    "plt.pie(counts, labels=labels, autopct='%.2f%%')\n",
    "del labels, counts\n",
    "plt.title(\"Images in each cluster\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration advice - some things to look for \n",
    "The following is a list of some general things that tend to be interesting within this notebooks visualisations.\n",
    "I would recommend investigating the following:\n",
    "\n",
    "\n",
    "1. The distribution of the nearest neighbour count histogram\n",
    "    - Some embeddings tend to be much more skewed than others\n",
    "\n",
    "2. Mean distance histogram \n",
    "    - Some distributions appear more normal than others.\n",
    "\n",
    "3. The difference in embedding between the most and least neighbour occurance images.\n",
    "    - Is one more even than the other? etc.\n",
    "\n",
    "4. The general theme of clusters (from setting `group_clusters` to not `False`)\n",
    "    - Some clusters have very explicit themes \n",
    "\n",
    "5. The 'arms' and 'clusters' in the force directed diagram\n",
    "    - Using the selection tool in the notebook to see if there are themes to arms and clusters\n",
    "    - Compare how these themes differ to the cluster themes \n",
    "    - What sort of images appear in the main hotspots?\n",
    "\n",
    "6. You can change the amount of neighbours for different advantages in the force directed diagram.\n",
    "    - Lower neighbours will create more branches, as it isolates small \"groups\"\n",
    "    - Higher neighbours will be more central, but accentuate the very different groups "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
