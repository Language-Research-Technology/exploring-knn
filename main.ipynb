{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Large Image Datasets with Image Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook builds a standalone visualisation of a set of images you upload in a zip file. It is inspired by the Image Machine [1]\n",
    "\n",
    "It will apply off-the-shelf algorithms for representing your images as a compact set of a numbers [(an embedding of the images in a vector space)](TODO: link to a wikipedia page). The resulting representation of the images is used to organise and visualise the relationships between the images you upload, *with respect to* the specific embedding algorithm chosen.  \n",
    "\n",
    "## Supported \"Embeddings\"\n",
    "\n",
    "This notebook supports generating the following embeddings for your uploaded images:\n",
    "\n",
    "- Colour Histogram: A size-normalised RGB pixel colour count array [[1]](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.histogram).\n",
    "- VGG (16 and 19): A convolutional neural network that returns probabilities for images being in each of the 1000 categories [[2]](https://pytorch.org/vision/main/models/vgg.html).\n",
    "- CLIP: OpenAi's model trainied on image-text pairs. [[3]](https://github.com/minimaxir/imgbeddings)\n",
    "\n",
    "## What You'll Need\n",
    "\n",
    "A `.zip` file containing images. This may contain nested folders, non-image files etc. as they will be filtered out. \n",
    "\n",
    "Estimated cell time table:\n",
    "| Cell Type    | Estimated runtime for 1000 images |\n",
    "| -------- | ------- |\n",
    "| Run Colour Histogram Embedding  | 15s    |\n",
    "| Run VGG19 Embedding | 13min     |\n",
    "| Run VGG16 Embedding    | 12min    |\n",
    "| Run CLIP Embedding    | 6min  |\n",
    "| Generate HTML for all 4 Embeddings    | 4min  |\n",
    "| Get KNN    | 30s  |\n",
    "\n",
    "### Don't Have a Dataset Yet?\n",
    "\n",
    "Try out these example datasets:\n",
    "\n",
    "- ***add links from zenodo + brief description***\n",
    "- ultrasounds images\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are familiar with jupyter notebooks please start running the notebook from [section 0](#0-import-dependencies), otherwise the following aims to give you a quickstart guide.\n",
    "\n",
    "1. Jupter notebooks are made up of 'cells,' these are either 'code' (python in this case ), or 'text' (markdown) cells.\n",
    "2. To run a cell you can either click on the run symbol in a cell, or use `shift`+`enter`.\n",
    "\n",
    "**add link to tutorial here from sam**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import Dependencies\n",
    "\n",
    "This notebook includes many functions in different files, feel free to explore them.\n",
    "\n",
    "First run the following cell (`shift`+`enter`) to import the required libraries/functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import numpy as np\n",
    "\n",
    "# Image processing\n",
    "from PIL import Image\n",
    "import os \n",
    "import zipfile\n",
    "\n",
    "### Embedding options ###\n",
    "## VGG16 ##\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import ImageDataset\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from torchvision import models\n",
    "\n",
    "## Plotting ##\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seeds\n",
    "import random\n",
    "random.seed(42) \n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Custom functions\n",
    "from get_embeddings import get_clip_embedding, get_normalised_histogram, get_embedding, get_embeddings_pytorch\n",
    "from embeddings_analysis import get_knn, get_count_neighbour_occurances, rank_neighbour_occurances, get_mean_distance, get_cluster_labels\n",
    "from force_directed_diagram import get_diagram_data #, get_position_df\n",
    "from get_html import get_html_rendering_details, generate_html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load images "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please upload a single `.zip` file into the `input` folder [here](./input), and then run the following cell to attempt to locate the image zip file.\n",
    "\n",
    "If you want to upload a new file, you will need to replace your existing uploaded file, or delete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = os.listdir('./input')\n",
    "if len(input_files) == 2:\n",
    "    if not (input_files[0].endswith('.zip') or input_files[1].endswith('.zip')):\n",
    "        print(\"Single file in input folder (excluding .gitkeep) is not a .zip file\")\n",
    "\n",
    "    else:\n",
    "        for file in input_files:\n",
    "            if file.endswith('.zip'):\n",
    "                # therefore there is one zip file \n",
    "                IMAGE_ZIP = \"./input/\" + file\n",
    "                print(f\"Successfully found file '{file}'.\")\n",
    "else:\n",
    "    print(\"One file not found in the input folder (excluding .gitkeep)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please run this cell to setup image parsing preferences, and to test image detections.\n",
    "\n",
    " You may edit `IMAGE_EXTENSIONS` if you wish to exclude/include particular filename suffixes. You may also edit `IMAGE_IGNORE` to exclude certain filename prefixes (e.g. `._` is a metadata filename prefix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust these to desired values (Zip file images will be read from)\n",
    "IMAGE_EXTENSIONS = (\".jpg\", \".JPG\", \".png\",\".PNG\", \".JPEG\", \".jpeg\") # The filename suffixes to be included\n",
    "IMAGE_IGNORE = (\"._\", ) # filename prefixes to be excluded \"._\" is for metadata\n",
    "\n",
    "# Display how many images are detected in the zip file.\n",
    "with zipfile.ZipFile(IMAGE_ZIP) as imgzip:\n",
    "    filenames = [name for name in imgzip.namelist() if name.endswith(IMAGE_EXTENSIONS) and not name.split('/')[-1].startswith(IMAGE_IGNORE)]\n",
    "    print(f\"{len(filenames)} images detected.\")\n",
    "    print(f\"First 5 filenames: {', '.join(filenames[:5])}\")\n",
    "del imgzip, filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is example code for displaying the first image in the zip file. Please run it to see your image and its filename appear below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(IMAGE_ZIP) as imgzip:\n",
    "    for name in imgzip.namelist():\n",
    "        if name.endswith(IMAGE_EXTENSIONS) and not name.split(\"/\")[-1].startswith(IMAGE_IGNORE):\n",
    "            with Image.open(imgzip.open(name)) as img:\n",
    "                print(name)\n",
    "                display(img)\n",
    "                break\n",
    "del imgzip, name, img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Embed images\n",
    "In this section we go over methods to convert our images into a 1D array of numbers (or an N-dimensional vector). There are many ways that this is done, and this notebook aims to allow for explicit comparisons between how different embedding methods \"see\" images.\n",
    "\n",
    "The following are simply examples, and you are more than welcome to adjust the existing code to fit your needs, or to import your own embeddings. The format is a 2D array, where each row is a different image (with the ith row in `embedding` corresponding to the ith filename in `image_filenames`).\n",
    "\n",
    "The purpose of sections 2.1 and 2.2 are to describe how the existing functions work for someone who wishes to modify the code. For those who don't wish to do this they may skip to section [2.3](#23-run-all-embeddings).\n",
    "\n",
    "The custom functions described in this section can be found in [get_embeddings.py](./get_embeddings.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Basic no-batch embedding\n",
    "The following provides code examples for how to do simple no-batch embedding. This means each image is iterated over individually. This will work in almost all cases, however may not be the most efficient. The `get_embedding` function does utilise multithreading.\n",
    "\n",
    "The following is an example of how to use the `get_embedding` function. It takes 3 parameters, the two constants defined above, and an `embedding_function`. This function must take in a `PIL.Image` object and return a 1D array of floats (it is crucial that this array is the same length for every image).\n",
    "\n",
    "The example `embedding_function` provided is `get_normalised_histogram`, but this can be swapped out to whatever you wish as long as it fulfills the above criteria.\n",
    "\n",
    "(This code is not crucial to the notebook, so you are welcome to set `skip` to `True` if you wish.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should take about 15 seconds to run for 1000 images\n",
    "skip = True\n",
    "if not skip:\n",
    "    embedding_function = get_normalised_histogram\n",
    "    embedding, image_filenames = get_embedding(image_zip=IMAGE_ZIP, \n",
    "                                            image_extensions=IMAGE_EXTENSIONS, \n",
    "                                            image_ignore=IMAGE_IGNORE,\n",
    "                                            embedding_function=embedding_function) \n",
    "    print(embedding[0:5], image_filenames[0:5])\n",
    "    del embedding, image_filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Batch embedding (pytorch)\n",
    "The following will describe how to utilise the existing code for pytorch batch embedding. \n",
    "\n",
    "This requires a custom dataset class and dataloader (defined below). It is recommended to play with the `DataLoader` parameters to see what runs best. If you use a pytorch embedder other than one of the VGG's (e.g. VGG16/VGG19) you may have to adjust the `ImageDataset` `transform` attribute to resize to an appropriate size with an appropriate mean and std of that embedder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is skipped by default as 1. It takes a while to run and 2. We will be running it later to produce some of the embeddings.\n",
    "\n",
    "Note: to adjust the model you may simply swap vgg16 to another pytorch model, listed here: https://pytorch.org/vision/main/models.html\n",
    "(Please keep in mind the `ImageDataset` `transform`s may need to be changed if you use a non VGG model, so please refer to the documentation for that model.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should take about 13 minutes to run for 1000 images\n",
    "skip = True\n",
    "if not skip:\n",
    "    \n",
    "    # Import custom tensor dataset from other file\n",
    "    dataset = ImageDataset(image_zip=IMAGE_ZIP, image_extensions=IMAGE_EXTENSIONS, image_ignore=IMAGE_IGNORE,\n",
    "                        transform= transforms.Compose([\n",
    "                            transforms.Resize((224, 224)),  # Resize image to 224x224 pixels\n",
    "                            transforms.ToTensor(),         # Convert image to tensor\n",
    "                            transforms.Normalize(          # Normalize using ImageNet statistics\n",
    "                                mean=[0.485, 0.456, 0.406],  # Mean for RGB channels\n",
    "                                std=[0.229, 0.224, 0.225]    # Standard deviation for RGB channels\n",
    "                                    )\n",
    "        ]))\n",
    "\n",
    "    # can look into optimising these values\n",
    "    dataloader = DataLoader(dataset, batch_size=64,\n",
    "                            shuffle=False, num_workers=0) #,prefetch_factor=2)\n",
    "\n",
    "    pytorch_model = models.vgg16(weights=models.VGG16_Weights.DEFAULT).to(\"cpu\")\n",
    "    batch_embeddings, image_filenames = get_embeddings_pytorch(dataloader, pytorch_model=pytorch_model)\n",
    "\n",
    "    print(embedding[0:5], image_filenames[0:5])\n",
    "    del embedding, image_filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Run all embeddings\n",
    "<!-- This cell runs the 4 default embeddings setup for this notebook. You are more than welcome to adjust this. The CLIP embedding is utilising the `imgbeddings` library: https://github.com/minimaxir/imgbeddings -->\n",
    "\n",
    "Run the cells you wish to generate embeddings for, this may be all of them, or only some.\n",
    "\n",
    "The embeddings will only be generated if they do not already exist for the current `IMAGE_ZIP` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Generate Colour Histogram Embeddings\n",
    "This should take about 15 seconds to run for 1000 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# get filenames from image_zip\n",
    "with zipfile.ZipFile(IMAGE_ZIP) as imgzip:\n",
    "        image_zip_filenames = [name for name in imgzip.namelist(\n",
    "        ) if name.endswith(IMAGE_EXTENSIONS) and not name.split(\"/\")[-1].startswith(IMAGE_IGNORE)]\n",
    "\n",
    "try:\n",
    "        image_filenames = np.load(\"./output/image_filenames_hist.npy\") # try and get premade filenames\n",
    "        if all(image_filenames == image_zip_filenames): # check if the filenames are the same as current\n",
    "                print(\"Embeddings already generated: passing.\")\n",
    "                \n",
    "        else:\n",
    "                print(\"Image filenames dont match\")\n",
    "                raise\n",
    "except: # the embeddings need to be regenerated\n",
    "        #### Get embedding without batching ####\n",
    "        embedding_function = get_normalised_histogram\n",
    "        embedding, image_filenames = get_embedding(image_zip=IMAGE_ZIP, image_extensions=IMAGE_EXTENSIONS, image_ignore=IMAGE_IGNORE, embedding_function=embedding_function) \n",
    "        np.save(\"./output/image_embedding_hist.npy\", embedding)\n",
    "        np.save(\"./output/image_filenames_hist.npy\", image_filenames) # this is to keep a consistent log of the index\n",
    "        print(\"Generated new embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Generate VGG19 Embeddings\n",
    "This should take about 13 minutes to run for 1000 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#### Get embedding with batching (for pytorch) #####\n",
    "# https://pytorch.org/vision/main/models/vgg.html\n",
    "# https://pytorch.org/vision/main/models/generated/torchvision.models.vgg16.html#torchvision.models.VGG16_Weights \n",
    "# get filenames from image_zip\n",
    "with zipfile.ZipFile(IMAGE_ZIP) as imgzip:\n",
    "        image_zip_filenames = [name for name in imgzip.namelist(\n",
    "        ) if name.endswith(IMAGE_EXTENSIONS) and not name.split(\"/\")[-1].startswith(IMAGE_IGNORE)]\n",
    "\n",
    "try:\n",
    "        image_filenames = np.load(\"./output/image_filenames_vgg19.npy\") # try and get premade filenames\n",
    "        if all(image_filenames == image_zip_filenames): # check if the filenames are the same as current\n",
    "                print(\"Embeddings already generated: passing.\")\n",
    "                \n",
    "        else:\n",
    "                print(\"Image filenames dont match\")\n",
    "                raise\n",
    "except: # the embeddings need to be regenerated\n",
    "    pytorch_model = models.vgg19(weights=models.VGG19_Weights.DEFAULT).to(\"cpu\")\n",
    "    batch_embeddings, image_filenames = get_embeddings_pytorch(dataloader, pytorch_model=pytorch_model)\n",
    "    np.save(\"./output/image_embedding_vgg19.npy\", batch_embeddings)\n",
    "    np.save(\"./output/image_filenames_vgg19.npy\", image_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3 Generate VGG16 Embeddings\n",
    "This should take about 12 minutes to run for 1000 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#### Get embedding with batching (for pytorch) #####\n",
    "# https://pytorch.org/vision/main/models/vgg.html\n",
    "# https://pytorch.org/vision/main/models/generated/torchvision.models.vgg16.html#torchvision.models.VGG16_Weights \n",
    "# get filenames from image_zip\n",
    "with zipfile.ZipFile(IMAGE_ZIP) as imgzip:\n",
    "        image_zip_filenames = [name for name in imgzip.namelist(\n",
    "        ) if name.endswith(IMAGE_EXTENSIONS) and not name.split(\"/\")[-1].startswith(IMAGE_IGNORE)]\n",
    "\n",
    "try:\n",
    "        image_filenames = np.load(\"./output/image_filenames_vgg16.npy\") # try and get premade filenames\n",
    "        if all(image_filenames == image_zip_filenames): # check if the filenames are the same as current\n",
    "                print(\"Embeddings already generated: passing.\")\n",
    "                \n",
    "        else:\n",
    "                print(\"Image filenames dont match\")\n",
    "                raise\n",
    "except: # the embeddings need to be regenerated\n",
    "    pytorch_model = models.vgg16(weights=models.VGG16_Weights.DEFAULT).to(\"cpu\")\n",
    "    batch_embeddings, image_filenames = get_embeddings_pytorch(dataloader, pytorch_model=pytorch_model)\n",
    "    np.save(\"./output/image_embedding_vgg16.npy\", batch_embeddings)\n",
    "    np.save(\"./output/image_filenames_vgg16.npy\", image_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.4 Generate CLIP Embeddings\n",
    "This should take about 6 minutes to run for 1000 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# get filenames from image_zip\n",
    "with zipfile.ZipFile(IMAGE_ZIP) as imgzip:\n",
    "        image_zip_filenames = [name for name in imgzip.namelist(\n",
    "        ) if name.endswith(IMAGE_EXTENSIONS) and not name.split(\"/\")[-1].startswith(IMAGE_IGNORE)]\n",
    "\n",
    "try:\n",
    "        image_filenames = np.load(\"./output/image_filenames_clip.npy\") # try and get premade filenames\n",
    "        if all(image_filenames == image_zip_filenames): # check if the filenames are the same as current\n",
    "                print(\"Embeddings already generated: passing.\")\n",
    "                \n",
    "        else:\n",
    "                print(\"Image filenames dont match\")\n",
    "                raise\n",
    "except: # the embeddings need to be regenerated\n",
    "    clip_embeddings, image_filenames = get_clip_embedding(IMAGE_ZIP, IMAGE_EXTENSIONS, image_ignore=IMAGE_IGNORE, batch_size=32)\n",
    "    np.save(\"./output/image_embedding_clip.npy\", clip_embeddings)\n",
    "    np.save(\"./output/image_filenames_clip.npy\", image_filenames) # this is to keep a consistent log of the index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. HTML Export \n",
    "Create a \"summary\" HTML document. What this document contains is better described within the file. If running in binderhub, please right click + `Open in New Browser Tab` the html file to ensure the javascript runs as expected. \n",
    "\n",
    "The main options are the following: `colour_map` (the variable that describes how different clusters are coloured) is set to `\"tab20\"` by default but it may be set to any of the options avaliable when running `from matplotlib import colormaps` and `list(colormaps)` (for over 20 clusters you would need to use a continous map like `\"viridis\"`).\n",
    "\n",
    "You may also select if you would like your images to be grouped by cluster when displayed. The `group_cluster` parameter has the following options:\n",
    "- `False`: Sort by rank normally\n",
    "- `True`: Order by Cluster label order\n",
    "- `asc`: Sort by ascending cluster size order\n",
    "- `desc`: Sort by descending cluster size order\n",
    "- `rank`: Sort by the highest ranked image in each cluster\n",
    "\n",
    "Please note the HTML is fully self contained, meaning after it is created it has the images and required packages encoded within it. This is so the HTML can be easily shared, and will still be runnable without internet access.\n",
    "\n",
    "The custom functions described in this section can be found in [get_html.py](./get_html.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell you can change the boolean value of `generate_all` to select one of the following:\n",
    "- `True`: generate a html file for each embedding in `embedding_name_list`.Note that the name in `embedding_name_list` refers to the name used when saving the embedding + image_filenames. This can be run immediately after saving the embeddings, and does not require any of the cells inbetween. \n",
    "\n",
    "- `False`: generate a single html file using the values from sections 2 and 3. This means it will use the embedding type you chose to load, the number of clusters or nearest neighbours you selected etc. This option requires running the cells up to this point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[filename[16:-4] for filename in os.listdir(\"./output\") if filename.startswith(\"image_embedding_\") and filename.endswith(\".npy\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Should take about 4 minutes to run for 1000 images and 4 embeddings.\n",
    "\n",
    "\n",
    "## Settings ##\n",
    "group_clusters='rank'\n",
    "colour_map=\"tab20\"\n",
    "number_of_neighbours=10\n",
    "n_clusters=20\n",
    "\n",
    "# detect existing embeddings\n",
    "embedding_name_list = [filename[16:-4] for filename in os.listdir(\"./output\") \n",
    "                if filename.startswith(\"image_embedding_\") and filename.endswith(\".npy\")]\n",
    "\n",
    "\n",
    "for embedding_name in embedding_name_list:\n",
    "    html_file = generate_html(*get_html_rendering_details(embedding_name, IMAGE_ZIP, IMAGE_EXTENSIONS, IMAGE_IGNORE, number_of_neighbours=number_of_neighbours, n_clusters=n_clusters), \n",
    "            group_clusters=group_clusters, \n",
    "            colour_map=colour_map)\n",
    "\n",
    "    with open(f\"./output/output_{embedding_name}.html\",'w') as file:\n",
    "        file.write(html_file)\n",
    "        del html_file, file\n",
    "\n",
    "    print(f\"Completed export for {embedding_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have generated the html output files, they can be found in [./output](./output). If running in binderhub, please right click + `Open in New Browser Tab` the html file to ensure the javascript runs as expected. Or you may download them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Load selected embeddings\n",
    "You may change the value of `embedding_to_load` to load a specific embedding of your choice. This choice will be used when walking through helper functions and visualisations in this notebook.\n",
    "\n",
    "Please note in section 4 of this notebook there is an option to either generate an output HTML file for just the embedding selected here, or for any number of them at once. If you opt for the second option and don't wish to see examples, you may [skip to generating html](#4-html-export) from here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# this may be either \"hist\", \"vgg16\", \"vgg19\", or \"clip\"\n",
    "embedding_to_load = \"hist\" \n",
    "#### Load Embeddings ####\n",
    "## Colour Histogram ##\n",
    "if embedding_to_load == \"hist\":\n",
    "    embedding = np.load(\"./output/image_embedding_hist.npy\")\n",
    "    image_filenames = np.load(\"./output/image_filenames_hist.npy\")\n",
    "## VGG16 ##\n",
    "elif embedding_to_load == \"vgg16\":\n",
    "    embedding = np.load(\"./output/image_embedding_vgg16.npy\")\n",
    "    image_filenames = np.load(\"./output/image_filenames_vgg16.npy\")\n",
    "## VGG19 ##\n",
    "elif embedding_to_load == \"vgg19\":\n",
    "    embedding = np.load(\"./output/image_embedding_vgg19.npy\")\n",
    "    image_filenames = np.load(\"./output/image_filenames_vgg19.npy\")\n",
    "## CLIP ##\n",
    "elif embedding_to_load == \"clip\":\n",
    "    embedding = np.load(\"./output/image_embedding_clip.npy\")\n",
    "    image_filenames = np.load(\"./output/image_filenames_clip.npy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Embedding Analysis\n",
    "In this section we will define some functions to help us analyse the image embeddings. The custom functions described in this section can be found in [embeddings_analysis.py](./embeddings_analysis.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Get KNN\n",
    "In this section we generate the `nearest_neighbours_array` and `neighbours_distance_array` for the desired `number_of_neighbours`. \n",
    "The `nearest_neighbours_array` is a 2D array where the ith row represents the ith filename in `image_filenames`, and each row lists the desired N nearest neighbours for that specific image.\n",
    "The `neighbours_distance_array` is similar to the `nearest_neighbours_array`, however lists the neigbour distance.\n",
    "\n",
    "Below is an example implementation, it shouldn't take long to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# This should take about 30 seconds to run for 1000 images\n",
    "number_of_neighbours = 10\n",
    "nearest_neighbours_array, neighbours_distance_array = get_knn(embedding, number_of_neighbours)\n",
    "\n",
    "nearest_neighbours_array[0:3], neighbours_distance_array[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Make some analysis helper functions\n",
    "The first helper function is `get_count_neighbour_occurances`. This function takes the `nearest_neighbour_arary` and returns an array where the ith index is how many times that index appeared as a neighbour (including as a neighbour of itself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_neighbour_occurance_array = get_count_neighbour_occurances(nearest_neighbours_array=nearest_neighbours_array)\n",
    "count_neighbour_occurance_array[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up we have `rank_neighbour_occurances`, which takes the previously defined `count_neighbour_occurance_array` as an input, and returns an array where the ith value in the array indicates that the index stored there is the ith most common or ith least common neighbour, depending on if the `ascending` parameter is set to `True` or `False` (`True` by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_neighbour_occurance_array = rank_neighbour_occurances(count_neighbour_occurance_array, ascending=True)\n",
    "print(f\"The most common neighbour is at image_filenames index {ranked_neighbour_occurance_array[-1]}\") # The last value as ascending\n",
    "ranked_neighbour_occurance_array[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_mean_distance` takes in the previously described `neighbours_distance_array` and returns a 1D array where the ith index indicates the ith images mean distance from all of its N nearest neighburs. You may choose whether this calculation should include itself as a neighbour (as each image is always its own closest neighbour) by setting the boolean value of `exclude_self` (`True` by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_mean_distance(neighbours_distance_array, exclude_self=True)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Getting K-means clusters\n",
    "These will be used as an additional visualisation option for the images.\n",
    "\n",
    "The `get_cluster_labels` function takes in `embedding` (the image embedding array) and `n_clusters` (desired number of clusters, by default 10). If you wish you may also change the `random_state`. This function returns a 1D array, where the ith index indicates what cluster the ith image lies in. (e.g. 0th cluster, 1st cluster...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cluster_labels = get_cluster_labels(embedding, n_clusters=20)\n",
    "cluster_labels[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Data exploration\n",
    "This following section aims to serve as an example of some visualisation options. Feel free to add your own here. \n",
    "Please note this section depends on the running of sections 2 and 3.\n",
    "\n",
    "First, lets graph the most common neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Most common neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_neighbour_occurances_array = get_count_neighbour_occurances(nearest_neighbours_array)\n",
    "ranked_neighbour_occurances = rank_neighbour_occurances(count_neighbour_occurances_array, ascending=False)\n",
    "\n",
    "n_most_common = 10\n",
    "x=[image_filenames[idx] for idx in ranked_neighbour_occurances[0:n_most_common]]\n",
    "y=[count_neighbour_occurances_array[idx] -1 for idx in ranked_neighbour_occurances[0:n_most_common]] # the -1 is to exclude counting being its own neighbour\n",
    "del count_neighbour_occurances_array, ranked_neighbour_occurances\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "sns.barplot(x=x, y=y)\n",
    "\n",
    "plt.xticks(rotation=270)\n",
    "plt.xlabel(\"Image names\")\n",
    "plt.ylabel(\"Neighbour Counts (excluding being its own neighbour)\")\n",
    "plt.title(f\"{n_most_common} most common image neighbours (descending) for 10 nearest neighbours (9 excluding self)\")\n",
    "\n",
    "plt.show()\n",
    "print(x[0]) # the filename for the most common image "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the most common image neighbour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(IMAGE_ZIP) as imgzip:\n",
    "    with Image.open(imgzip.open(x[0])) as img:\n",
    "        display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Closest image to its neighbours (average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours_mean_distance_array = get_mean_distance(neighbours_distance_array)\n",
    "ranked_neighbour_occurances = rank_neighbour_occurances(neighbours_mean_distance_array, ascending=True)\n",
    "\n",
    "n_most_common = 10\n",
    "x=[image_filenames[idx] for idx in ranked_neighbour_occurances[0:n_most_common]]\n",
    "y=[neighbours_mean_distance_array[idx] for idx in ranked_neighbour_occurances[0:n_most_common]] # the -1 is to exclude counting being its own neighbour\n",
    "del neighbours_mean_distance_array, ranked_neighbour_occurances\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "sns.barplot(x=x, y=y)\n",
    "\n",
    "plt.xticks(rotation=270)\n",
    "plt.xlabel(\"Image names\")\n",
    "plt.ylabel(\"Mean distance from neighbours (excluding itself)\")\n",
    "plt.title(f\"{n_most_common} images with the smallest mean distance from 10 nearest neighbours (excluding self)\")\n",
    "\n",
    "plt.show()\n",
    "print(x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the closest image neighbour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(IMAGE_ZIP) as imgzip:\n",
    "    with Image.open(imgzip.open(x[0])) as img:\n",
    "        display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Histogram for times an image appears as a nearest neighbour\n",
    "This graph is worth noting in particular for different embeddings, some have much more even distributions than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_count_neighbour_occurances_array = np.sort(get_count_neighbour_occurances(nearest_neighbours_array))-1 # -1 to exclude self\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "sns.histplot(x=sorted_count_neighbour_occurances_array, kde=True, bins=30)\n",
    "plt.xticks(rotation=270)\n",
    "plt.xlabel(f\"Number of nearest neighbour occurances excluding self ({number_of_neighbours} nearest neighbours)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(f\"Nearest neighbour count histogram for {number_of_neighbours} nearest neighbours\")\n",
    "\n",
    "plt.show()\n",
    "del sorted_count_neighbour_occurances_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Histogram for an images mean distance across its neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = False\n",
    "\n",
    "if log:\n",
    "    sorted_mean_distance_array = np.log(np.sort(get_mean_distance(neighbours_distance_array)))\n",
    "    title = \"Log mean\"\n",
    "else:\n",
    "    sorted_mean_distance_array = np.sort(get_mean_distance(neighbours_distance_array))\n",
    "    title = \"Mean\"\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "sns.histplot(x=sorted_mean_distance_array, kde=True, bins=30)\n",
    "\n",
    "plt.xticks(rotation=270)\n",
    "plt.xlabel(f\"{title} distance from nearest neighbours excluding self ({number_of_neighbours} nearest neighbours)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(f\"{title} neighbour distance histogram for {number_of_neighbours} nearest neighbours\")\n",
    "\n",
    "plt.show()\n",
    "del sorted_mean_distance_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Visualise the embeddings for the most and least common nearest neighbours\n",
    "\n",
    "A bar graph to physically view the differences between the most and least common image neighbours. Please note these graphs don't always work due to handling of extreme values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_neighbour_occurances_array = get_count_neighbour_occurances(nearest_neighbours_array)\n",
    "max_index = np.argmax(count_neighbour_occurances_array)\n",
    "\n",
    "plt.bar(np.arange(len(embedding[max_index])), embedding[max_index])\n",
    "plt.title(\"Embedding for highest neighbour image\")\n",
    "plt.show()\n",
    "print(image_filenames[max_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_index = np.argmin(count_neighbour_occurances_array)\n",
    "\n",
    "plt.bar(np.arange(len(embedding[min_index])), embedding[min_index])\n",
    "plt.title(\"Embedding for lowest neighbour image\")\n",
    "plt.show()\n",
    "del count_neighbour_occurances_array\n",
    "print(image_filenames[min_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Force directed diagram code\n",
    "This section was originally to create an interactive \"network plot\" with a heatmap for aid in visualisation, using different colours for each points cluster. \n",
    "\n",
    "The code for viewing these plots can be seen commented out in [force_directed_diagram.py](./force_directed_diagram.py), however the same visualisation are avaliable in the output html documents [created in section 4](#4-html-export) (**this is the recommended way to view these graphs**).\n",
    "\n",
    "The example code shown in this section is use to create the diagrams for the output html file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_diagram_data` function returns all the data needed to construct the force directed diagram, taking in a variety of previously defined data. Setting `return_json` to `True` will return a string (this is used for generating the HTML)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagram_data = get_diagram_data(nearest_neighbours_array, neighbours_distance_array, cluster_labels, image_filenames, return_json=False)\n",
    "print(diagram_data.keys()) # showing keys as the actual dictionary is very large\n",
    "del diagram_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7 Pie chart for images in each cluster\n",
    "This chart was created to help visualise the differences in cluster sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, counts = np.unique(cluster_labels, return_counts=True) \n",
    "\n",
    "plt.pie(counts, labels=labels, autopct='%.2f%%')\n",
    "del labels, counts\n",
    "plt.title(\"Images in each cluster\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration advice - some things to look for \n",
    "The following is a list of some general things that tend to be interesting within this notebooks visualisations.\n",
    "I would recommend investigating the following:\n",
    "\n",
    "\n",
    "1. The distribution of the nearest neighbour count histogram\n",
    "    - Some embeddings tend to be much more skewed than others\n",
    "\n",
    "2. Mean distance histogram \n",
    "    - Some distributions appear more normal than others.\n",
    "\n",
    "3. The difference in embedding between the most and least neighbour occurance images.\n",
    "    - Is one more even than the other? etc.\n",
    "\n",
    "4. The general theme of clusters (from setting `group_clusters` to not `False`)\n",
    "    - Some clusters have very explicit themes \n",
    "\n",
    "5. The 'arms' and 'clusters' in the force directed diagram\n",
    "    - Using the selection tool in the notebook to see if there are themes to arms and clusters\n",
    "    - Compare how these themes differ to the cluster themes \n",
    "    - What sort of images appear in the main hotspots?\n",
    "\n",
    "6. You can change the amount of neighbours for different advantages in the force directed diagram.\n",
    "    - Lower neighbours will create more branches, as it isolates small \"groups\"\n",
    "    - Higher neighbours will be more central, but accentuate the very different groups "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
